# Short-Term Traffic Flow Prediction with History-Enhanced LSTM
### åŸºäºå†å²å¢å¼º LSTM çš„çŸ­æ—¶äº¤é€šæµæ®‹å·®é¢„æµ‹

## ğŸ“– Project Overview (é¡¹ç›®æ¦‚è¿°)
This project aims to predict short-term traffic flow changes (residuals) for the next 30 minutes. 

Instead of relying solely on recent traffic data (which often leads to "lagging" predictions), this project introduces a **History-Enhanced Architecture**. By incorporating "Daily" (Yesterday) and "Weekly" (Last Week) contexts, the model captures underlying periodic trends, significantly improving prediction accuracy and robustness against noise.

**Key Achievement:** Reduced RMSE from **7.71** (Baseline) to **6.46** (SOTA Performance).

## ğŸ—ï¸ Model Architectures (æ¨¡å‹æ¶æ„)

We explored three different architectures to validate the hypothesis:

1.  **Baseline LSTM:** Standard LSTM using only recent data. (High reliance on inertia).
2.  **Multi-Branch Fusion Network:** * Three independent LSTM branches processing Recent, Daily, and Weekly patterns separately.
    * Uses a fusion layer to weight the contributions.
    * *Insight:* Highly effective at capturing Weekly seasonality.
3.  **Single-Stream Concatenated LSTM :**
    * Concatenates [Weekly + Daily + Recent] sequences into a single time-series input.
    * Uses Attention mechanism to identify relevant historical context.
    * *Result:* Best performance due to efficient fusion of Daily trends and recent fluctuations.

## ğŸ“Š Results & Performance (å®éªŒç»“æœ)

The models were evaluated on a strict **Weekly Split** (8 weeks train, 1 week val, remaining test).

| Model Architecture      | RMSE | RÂ² Score | Key Characteristic |
|:------------------------| :--- | :--- | :--- |
| **Baseline LSTM**       | 7.71 | 0.929 | High lag, sensitive to noise |
| **Multi-Branch Fusion** | 6.61 | 0.949 | Strong noise robustness, captures Weekly trend |
| **Single-Stream**       | **6.46** | **0.951** | **Lowest error, best trend-following capability** |

### Visualization (é¢„æµ‹æ•ˆæœå¯¹æ¯”)

**1. Champion Model (Single-Stream) vs Ground Truth:**
The model effectively filters high-frequency noise and follows the true trend.
![Single Stream Prediction](results/singlestream.png)

**2. Multi-Branch Fusion Prediction:**
![Multi Branch Prediction](results/multibranch.png)

## ğŸ” Feature Importance Analysis (æ ¸å¿ƒå‘ç°)

Why did the models improve? We used Permutation Feature Importance to look inside the "Black Box".

**Discovery 1: The Shift from Inertia to History**
* The Baseline model relied almost 100% on recent flow (Inertia).
* The improved models learned to utilize historical patterns significantly.

**Discovery 2: Daily vs. Weekly**
* **Multi-Branch Model:** Prioritized **Weekly** patterns (+23% Importance), treating weekends/weekdays differently.
* **Single-Stream Model:** Prioritized **Daily** patterns (+21% Importance), finding that "Yesterday" is often the best predictor for "Today" in this specific dataset.

![Feature Importance](results/multi_analysis.png)
![Feature Importance](results/single_analysis.png)

## ğŸ“‚ Project Structure (é¡¹ç›®ç»“æ„)

```text
Traffic_Prediction/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ station_407204_3months.csv  # Dataset
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model_baseline_lstm.py      # Baseline Model
â”‚   â”œâ”€â”€ model_multibranch.py        # 3-Branch Architecture
â”‚   â””â”€â”€ model_singlestream.py       # Best Performing Model
â”‚
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ multi_analysis.py           # Feature Importance for Multi-Branch
â”‚   â””â”€â”€ single_analysis.py          # Feature Importance for Single-Stream
â”‚
â”œâ”€â”€ checkpoint/
â”‚   â””â”€â”€ (Saved .pth models)
â”‚
â””â”€â”€ results/
    â””â”€â”€ (Visualization plots)