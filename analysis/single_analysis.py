import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import math
import copy
import random
import os

# ================= ğŸ”§ é…ç½® (å¿…é¡»ä¸ Script 8 å®Œå…¨ä¸€è‡´) =================
CSV_FILE_PATH = 'data/station_407204_3months.csv'
MODEL_PATH = 'checkpoint/champion_model.pth'  # ğŸ‘ˆ åŠ è½½å•æµæ¨¡å‹çš„æƒé‡

# --- ç‰©ç†åœºæ™¯ ---
NUM_LANES = 4
HORIZON = 6

# --- çª—å£å®šä¹‰ (ä¸²è”é€»è¾‘) ---
LEN_RECENT = 12
LEN_PERIOD = 24
# æ€»é•¿åº¦ = 24 (Week) + 24 (Day) + 12 (Recent) = 60

# --- è®­ç»ƒå‚æ•° ---
TRAIN_WEEKS = 8
VAL_WEEKS = 1

# --- æ¨¡å‹å‚æ•° ---
HIDDEN_SIZE = 128  # å¿…é¡»ä¸ Script 8 ä¸€è‡´
DROPOUT = 0.2

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")


def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)


seed_everything(42)


# ================= 1. æ•°æ®å‡†å¤‡  =================
def load_data_simple():
    print(f"ğŸš€ [Step 1] è¯»å–æ•°æ®...")
    df = pd.read_csv(CSV_FILE_PATH)
    df['Timestamp'] = pd.to_datetime(df['Timestamp'])
    df.sort_values('Timestamp', inplace=True)
    return df


def create_concatenated_dataset(data, timestamps, len_recent=12, len_period=24, horizon=6):
    X, Y = [], []
    valid_timestamps = []

    LAG_DAY = 288
    LAG_WEEK = 2016
    half_period = len_period // 2

    start_idx = LAG_WEEK + half_period
    end_idx = len(data) - horizon

    for i in range(start_idx, end_idx):
        current_flow = data[i, 0]
        future_flow = data[i + horizon - 1, 0]
        delta = future_flow - current_flow

        # 1. Weekly (Oldest)
        wk_center = i - LAG_WEEK
        wk_seq = data[wk_center - half_period: wk_center + half_period, 0]

        # 2. Daily (Middle)
        day_center = i - LAG_DAY
        day_seq = data[day_center - half_period: day_center + half_period, 0]

        # 3. Recent (Newest)
        rec_seq = data[i - len_recent + 1: i + 1, 0]

        if len(rec_seq) == len_recent and len(day_seq) == len_period and len(wk_seq) == len_period:
            # æ‹¼æ¥: [Week(24), Day(24), Recent(12)]
            combined_seq = np.concatenate((wk_seq, day_seq, rec_seq))
            X.append(combined_seq)
            Y.append(delta)
            valid_timestamps.append(timestamps[i + horizon - 1])

    return np.array(X), np.array(Y), np.array(valid_timestamps)


# ================= 2. æ¨¡å‹å®šä¹‰  =================
class SingleStreamLSTMAttention(nn.Module):
    def __init__(self, input_size=1, hidden_size=128, output_size=1, dropout=0.2):
        super(SingleStreamLSTMAttention, self).__init__()

        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=0)
        self.dropout_layer = nn.Dropout(dropout)

        self.attention_net = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1)
        )
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h_output, _ = self.lstm(x)
        attn_weights = F.softmax(self.attention_net(h_output), dim=1)
        context = self.dropout_layer(torch.sum(attn_weights * h_output, dim=1))
        out = self.fc(context)
        return out


# ================= 3. æ ¸å¿ƒï¼šå•æµåˆ‡ç‰‡é‡è¦æ€§åˆ†æ =================
def evaluate_model(model, t_X, t_Y, scaler, shuffle_part=None):
    """
    é€šè¿‡åˆ‡ç‰‡ç´¢å¼•æ¥æ‰“ä¹±ç‰¹å®šéƒ¨åˆ†
    Structure: [Week (0-24) | Day (24-48) | Recent (48-60)]
    """
    model.eval()

    # å¤åˆ¶è¾“å…¥
    X_in = t_X.clone()  # Shape: [Batch, 60, 1]

    # --- å…³é”®ï¼šæ ¹æ®ç´¢å¼•æ‰“ä¹± ---
    batch_size = X_in.size(0)
    idx = torch.randperm(batch_size)  # ç”Ÿæˆéšæœºç´¢å¼•

    if shuffle_part == 'week':
        # æ‰“ä¹±å‰ 24 ä¸ªæ—¶é—´æ­¥ (Index 0-24)
        # æ³¨æ„ï¼šæˆ‘ä»¬è¦ä¿æŒæ—¶é—´æ­¥å†…éƒ¨é¡ºåºä¸å˜ï¼Œåªåœ¨ Batch ä¹‹é—´äº¤æ¢
        # X_in[idx, :24, :] æŠŠä¹±åºçš„ Batch èµ‹ç»™åŸä½ç½®ï¼Œä½†è¿™æ ·å†™åœ¨ PyTorch é‡Œæ¯”è¾ƒå¤æ‚
        # æ›´ç®€å•çš„æ–¹æ³•ï¼šå–å‡ºè¯¥æ®µ -> æ‰“ä¹± -> æ”¾å›
        part = X_in[:, :24, :]
        X_in[:, :24, :] = part[idx]

    elif shuffle_part == 'day':
        # æ‰“ä¹±ä¸­é—´ 24 ä¸ªæ—¶é—´æ­¥ (Index 24-48)
        part = X_in[:, 24:48, :]
        X_in[:, 24:48, :] = part[idx]

    elif shuffle_part == 'recent':
        # æ‰“ä¹±æœ€å 12 ä¸ªæ—¶é—´æ­¥ (Index 48-60)
        part = X_in[:, 48:, :]
        X_in[:, 48:, :] = part[idx]

    with torch.no_grad():
        pred_delta_norm = model(X_in).cpu().numpy()

    # è¿˜åŸé€»è¾‘
    # Base Value å¿…é¡»æ˜¯ã€çœŸå®çš„ã€‘Current Flow
    # åœ¨ä¸²è”åºåˆ—ä¸­ï¼ŒCurrent Flow æ˜¯æœ€åä¸€ä¸ªç‚¹ (Index -1)
    # æ³¨æ„ï¼šå¿…é¡»ç”¨åŸå§‹ t_X å–å€¼ï¼Œä¸èƒ½ç”¨æ‰“ä¹±åçš„ X_in
    base_flow_norm = t_X[:, -1, 0].cpu().numpy().reshape(-1, 1)

    pred_flow_norm = base_flow_norm + pred_delta_norm
    true_delta_norm = t_Y.cpu().numpy()
    true_flow_norm = base_flow_norm + true_delta_norm

    pred_lane = scaler.inverse_transform(pred_flow_norm) / NUM_LANES
    true_lane = scaler.inverse_transform(true_flow_norm) / NUM_LANES

    rmse = math.sqrt(mean_squared_error(true_lane, pred_lane))
    return rmse


# ================= ä¸»ç¨‹åº =================
def run_single_stream_analysis():
    # 1. å‡†å¤‡æ•°æ®
    print("â³ æ­£åœ¨å‡†å¤‡æµ‹è¯•æ•°æ®...")
    df = load_data_simple()
    raw_flow = df['Flow'].values.reshape(-1, 1)
    timestamps = df['Timestamp'].values

    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_flow = scaler.fit_transform(raw_flow)

    X, Y, _ = create_concatenated_dataset(
        scaled_flow, timestamps,
        len_recent=LEN_RECENT,
        len_period=LEN_PERIOD,
        horizon=HORIZON
    )

    # å®šä½æµ‹è¯•é›†
    POINTS_PER_WEEK = 288 * 7
    train_pts = TRAIN_WEEKS * POINTS_PER_WEEK
    val_pts = VAL_WEEKS * POINTS_PER_WEEK

    test_X = X[train_pts + val_pts:]
    test_Y = Y[train_pts + val_pts:]

    print(f"ğŸ“Š åˆ†ææ ·æœ¬æ•°: {len(test_Y)}")

    t_X = torch.from_numpy(test_X).float().unsqueeze(2).to(device)
    t_Y = torch.from_numpy(test_Y).float().unsqueeze(1).to(device)

    # 2. åŠ è½½æ¨¡å‹
    print(f"ğŸš€ åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    model = SingleStreamLSTMAttention(input_size=1, hidden_size=HIDDEN_SIZE, dropout=DROPOUT).to(device)

    if os.path.exists(MODEL_PATH):
        try:
            model.load_state_dict(torch.load(MODEL_PATH))
            print("âœ… æƒé‡åŠ è½½æˆåŠŸï¼")
        except RuntimeError as e:
            print(f"âŒ æƒé‡åŠ è½½å¤±è´¥ï¼ç»“æ„ä¸åŒ¹é…ã€‚\n{e}")
            return
    else:
        print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼è¯·å…ˆè¿è¡Œ Script 8ã€‚")
        return

    # 3. è¿è¡Œåˆ†æ
    print("\nğŸ” å¼€å§‹å•æµæ¨¡å‹ç‰¹å¾é‡è¦æ€§æµ‹è¯•...")

    baseline_rmse = evaluate_model(model, t_X, t_Y, scaler, shuffle_part=None)
    print(f"âœ… Baseline RMSE: {baseline_rmse:.4f}")

    parts = [
        ('Recent Part (Last 12 steps)', 'recent'),
        ('Daily Part (Middle 24 steps)', 'day'),
        ('Weekly Part (First 24 steps)', 'week')
    ]

    results = []

    for name, code in parts:
        print(f"   ğŸ‘‰ æ‰“ä¹± [{name}] ...")
        shuffled_rmse = evaluate_model(model, t_X, t_Y, scaler, shuffle_part=code)

        diff = shuffled_rmse - baseline_rmse
        pct = (diff / baseline_rmse) * 100
        results.append((name, pct))
        print(f"      -> New RMSE: {shuffled_rmse:.4f} (+{pct:.2f}%)")

    # 4. ç”»å›¾
    print("\nğŸ† æœ€ç»ˆæ’å:")
    results.sort(key=lambda x: x[1], reverse=True)
    names = [x[0] for x in results]
    values = [x[1] for x in results]

    for n, v in results:
        print(f"   {n}: +{v:.2f}% Impact")

    plt.style.use('seaborn-v0_8-whitegrid')
    fig, ax = plt.subplots(figsize=(10, 6))

    colors = plt.cm.Greens(np.linspace(0.4, 0.9, len(names)))  # ç”¨ç»¿è‰²åŒºåˆ†å•æµ
    bars = ax.barh(names, values, color=colors)
    ax.invert_yaxis()

    ax.set_xlabel('% Increase in RMSE (Importance)', fontsize=12)
    ax.set_title('Single-Stream Model Feature Analysis\n(Impact of Shuffling Time Segments)', fontsize=14,
                 fontweight='bold')

    for bar in bars:
        width = bar.get_width()
        ax.text(width + 0.1, bar.get_y() + bar.get_height() / 2, f'+{width:.2f}%', va='center', fontweight='bold')

    plt.tight_layout()
    plt.show()


if __name__ == '__main__':
    run_single_stream_analysis()