import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
import math
import copy
import random
import os

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
CSV_FILE_PATH = r'D:\Traffic_Prediction\data\station_407204_3months.csv'
SAVE_MODEL_NAME = 'single_stream_model.pth'

# --- ç‰©ç†åœºæ™¯ ---
NUM_LANES = 4
HORIZON = 6  # é¢„æµ‹ 30åˆ†é’Ÿå

# --- çª—å£å®šä¹‰ (ä¸²è”é€»è¾‘) ---
LEN_RECENT = 12  # 1å°æ—¶
LEN_PERIOD = 24  # 2å°æ—¶ (å‰åå„1)
# æ€»åºåˆ—é•¿åº¦ = 24 + 24 + 12 = 60

# --- ä¸¥æ ¼åˆ‡åˆ†ç­–ç•¥ ---
TRAIN_WEEKS = 8
VAL_WEEKS = 1

# --- æ¨¡å‹å‚æ•° ---
BATCH_SIZE = 256  # å•æµæ¨¡å‹æ˜¾å­˜å ç”¨å°ï¼ŒBatchå¯ä»¥å¤§ç‚¹
EPOCHS = 200
LEARNING_RATE = 0.001
PATIENCE = 30
HIDDEN_SIZE = 128  # åºåˆ—é•¿äº†ï¼Œç”¨å¤§ä¸€ç‚¹çš„ Hidden Size
DROPOUT = 0.2

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")


def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


seed_everything(42)


# ================= 1. æ•°æ®å‡†å¤‡ =================
def load_data_simple():
    print(f"ğŸš€ [Step 1] è¯»å–æ•°æ®...")
    df = pd.read_csv(CSV_FILE_PATH)
    df['Timestamp'] = pd.to_datetime(df['Timestamp'])
    df.sort_values('Timestamp', inplace=True)
    return df


def create_concatenated_dataset(data, timestamps, len_recent=12, len_period=24, horizon=6):
    """
    æ„å»º [Week_Seq + Day_Seq + Recent_Seq] çš„é•¿åºåˆ—
    """
    X, Y = [], []
    valid_timestamps = []

    LAG_DAY = 288
    LAG_WEEK = 2016
    half_period = len_period // 2

    start_idx = LAG_WEEK + half_period
    end_idx = len(data) - horizon

    print(f"â³ æ­£åœ¨æ„å»ºä¸²è”æ•°æ®é›† (Concatenated Input)...")

    for i in range(start_idx, end_idx):
        # Y: Residual Target
        current_flow = data[i, 0]
        future_flow = data[i + horizon - 1, 0]
        delta = future_flow - current_flow

        # 1. Weekly Part (Oldest)
        wk_center = i - LAG_WEEK
        wk_seq = data[wk_center - half_period: wk_center + half_period, 0]

        # 2. Daily Part (Middle)
        day_center = i - LAG_DAY
        day_seq = data[day_center - half_period: day_center + half_period, 0]

        # 3. Recent Part (Newest)
        rec_seq = data[i - len_recent + 1: i + 1, 0]

        if len(rec_seq) == len_recent and len(day_seq) == len_period and len(wk_seq) == len_period:
            # --- æ ¸å¿ƒæ“ä½œï¼šæ‹¼æ¥ ---
            # é¡ºåº: [ä¸Šå‘¨(24) -> æ˜¨å¤©(24) -> ä»Šå¤©(12)]
            combined_seq = np.concatenate((wk_seq, day_seq, rec_seq))

            X.append(combined_seq)
            Y.append(delta)
            valid_timestamps.append(timestamps[i + horizon - 1])

    return np.array(X), np.array(Y), np.array(valid_timestamps)


# ================= 2. æ¨¡å‹å®šä¹‰ (å•æµ LSTM-Attention) =================
class SingleStreamLSTMAttention(nn.Module):
    def __init__(self, input_size=1, hidden_size=128, output_size=1, dropout=0.2):
        super(SingleStreamLSTMAttention, self).__init__()

        # LSTM å¤„ç†é•¿åºåˆ— (Seq_Len=60)
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=0)
        self.dropout_layer = nn.Dropout(dropout)

        # Attention
        self.attention_net = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1)
        )

        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # x shape: [Batch, 60, 1]

        # 1. LSTM æå–ç‰¹å¾
        h_output, _ = self.lstm(x)  # [Batch, 60, Hidden]

        # 2. Attention è®¡ç®—æƒé‡
        # è¿™ä¸€æ­¥å°±æ˜¯åœ¨ 60 ä¸ªæ—¶é—´æ­¥é‡Œæ‰¾é‡ç‚¹
        attn_weights = self.attention_net(h_output)
        attn_weights = F.softmax(attn_weights, dim=1)

        # 3. åŠ æƒæ±‚å’Œ
        context = torch.sum(attn_weights * h_output, dim=1)
        context = self.dropout_layer(context)

        out = self.fc(context)
        return out


# ================= 3. ä¸»ç¨‹åº =================
def run_concatenated_final():
    # 1. å‡†å¤‡æ•°æ®
    df = load_data_simple()
    raw_flow = df['Flow'].values.reshape(-1, 1)
    timestamps = df['Timestamp'].values

    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_flow = scaler.fit_transform(raw_flow)

    # æ„å»ºä¸²è”æ•°æ®é›†
    X, Y, Y_times = create_concatenated_dataset(
        scaled_flow, timestamps,
        len_recent=LEN_RECENT,
        len_period=LEN_PERIOD,
        horizon=HORIZON
    )

    # 2. ä¸¥æ ¼æŒ‰å‘¨åˆ‡åˆ†
    POINTS_PER_WEEK = 288 * 7
    train_pts = TRAIN_WEEKS * POINTS_PER_WEEK
    val_pts = VAL_WEEKS * POINTS_PER_WEEK

    total_samples = len(Y)
    if train_pts + val_pts > total_samples:
        raise ValueError(f"æ•°æ®ä¸è¶³ï¼éœ€è¦ {train_pts + val_pts}, åªæœ‰ {total_samples}")

    print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ† (Strict Weekly): Train={train_pts}, Val={val_pts}, Test={total_samples - train_pts - val_pts}")
    print(f"   è¾“å…¥åºåˆ—é•¿åº¦: {X.shape[1]} (24+24+12)")

    t_X = torch.from_numpy(X).float().unsqueeze(2).to(device)  # [N, 60, 1]
    t_Y = torch.from_numpy(Y).float().unsqueeze(1).to(device)

    # DataLoader
    train_data = torch.utils.data.TensorDataset(t_X[:train_pts], t_Y[:train_pts])
    val_data = (t_X[train_pts:train_pts + val_pts], t_Y[train_pts:train_pts + val_pts])
    test_data = (t_X[train_pts + val_pts:], t_Y[train_pts + val_pts:])
    test_timestamps = Y_times[train_pts + val_pts:]

    train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)

    # 3. è®­ç»ƒ
    model = SingleStreamLSTMAttention(input_size=1, hidden_size=HIDDEN_SIZE, dropout=DROPOUT).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

    best_val_loss = float('inf')
    best_weights = copy.deepcopy(model.state_dict())
    patience_cnt = 0

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒå•æµä¸²è”æ¨¡å‹... (ä¿å­˜ä¸º {SAVE_MODEL_NAME})")

    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        for bx, by in train_loader:
            optimizer.zero_grad()
            out = model(bx)
            loss = criterion(out, by)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)

        model.eval()
        with torch.no_grad():
            vx, vy = val_data
            val_out = model(vx)
            val_loss = criterion(val_out, vy).item()

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_weights = copy.deepcopy(model.state_dict())
            patience_cnt = 0
            torch.save(model.state_dict(), SAVE_MODEL_NAME)  # è‡ªåŠ¨ä¿å­˜
        else:
            patience_cnt += 1
            if patience_cnt >= PATIENCE:
                print(f"ğŸ›‘ Early stopping at epoch {epoch + 1}")
                break

        if (epoch + 1) % 10 == 0:
            print(f"   Epoch {epoch + 1} | Train: {avg_train_loss:.5f} | Val: {val_loss:.5f}")

    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {SAVE_MODEL_NAME}")

    # 4. è¯„ä¼°
    model.load_state_dict(best_weights)
    model.eval()

    tx, ty = test_data
    with torch.no_grad():
        pred_delta_norm = model(tx).cpu().numpy()
        true_delta_norm = ty.cpu().numpy()

    # è¿˜åŸ (Base Value æ˜¯ Recent éƒ¨åˆ†çš„æœ€åä¸€ä¸ªç‚¹ï¼Œå³åºåˆ—çš„æœ€åä¸€ä¸ªç‚¹ -1)
    # X shape: [N, 60], Last point is current flow
    base_flow_norm = X[train_pts + val_pts:, -1].reshape(-1, 1)

    pred_flow_norm = base_flow_norm + pred_delta_norm
    true_flow_norm = base_flow_norm + true_delta_norm

    pred_total = scaler.inverse_transform(pred_flow_norm)
    true_total = scaler.inverse_transform(true_flow_norm)
    pred_lane = pred_total / NUM_LANES
    true_lane = true_total / NUM_LANES

    rmse = math.sqrt(mean_squared_error(true_lane, pred_lane))
    r2 = r2_score(true_lane, pred_lane)

    print("-" * 50)
    print(f"ğŸ”¥ Single-Stream Concatenated Result:")
    print(f"   RMSE: {rmse:.2f}")
    print(f"   RÂ²:   {r2:.4f}")
    print("-" * 50)

    # 5. å¯è§†åŒ– (å¸¦å¹³æ»‘)
    plt.style.use('seaborn-v0_8-whitegrid')
    df_res = pd.DataFrame({
        'Time': pd.to_datetime(test_timestamps),
        'True': true_lane.flatten(),
        'Pred': pred_lane.flatten()
    })

    # è®¡ç®—å¹³æ»‘è¶‹åŠ¿ (Rolling Mean)
    df_res['True_Smooth'] = df_res['True'].rolling(window=3, center=True, min_periods=1).mean()

    thursdays = df_res[df_res['Time'].dt.dayofweek == 3]['Time'].dt.date.unique()
    if len(thursdays) > 0:
        target_date = thursdays[-1]
        plot_data = df_res[df_res['Time'].dt.date == target_date]

        fig, ax = plt.subplots(figsize=(12, 6))

        # åŸå§‹æ•°æ® (æµ…è‰²)
        ax.plot(plot_data['Time'], plot_data['True'], label='Observed (Raw)', color='lightgray', alpha=0.5, linewidth=1)
        # å¹³æ»‘è¶‹åŠ¿ (æ·±è‰²)
        ax.plot(plot_data['Time'], plot_data['True_Smooth'], label='Observed (Smoothed)', color='gray', alpha=0.8,
                linewidth=2)
        # é¢„æµ‹å€¼ (ç»¿è‰²ï¼Œä»¥ä¾¿åŒºåˆ†ä¹‹å‰çš„æ©™è‰²)
        ax.plot(plot_data['Time'], plot_data['Pred'], label='Single-Stream Prediction', color='#2ecc71', linestyle='--',
                linewidth=2)

        ax.set_title(f'Single-Stream Concatenated (Week+Day+Recent)\nRMSE: {rmse:.2f}, RÂ²: {r2:.3f}', fontsize=14,
                     fontweight='bold')
        ax.set_ylabel('Flow Rate (veh/5min/lane)', fontsize=12)
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))
        ax.legend()
        plt.tight_layout()
        plt.show()


if __name__ == '__main__':
    run_concatenated_final()